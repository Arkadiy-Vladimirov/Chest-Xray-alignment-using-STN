experiment_path: /home/orogov/smbmount/a_galichin/experiments/alignment
experiment_name: nih_resnet18_densenet121_320x320_perceptual_bs16
trainer:
  devices:
  - 0
  - 1
  strategy: ddp
  precision: 32
  max_epochs: 3
data:
  dataset:
    _target_: src.data.dataset.ChestXRayAlignmentDataset
    main_dir: /home/orogov/smbmount/from_DGX/cxr14-2
    transforms:
    - _target_: albumentations.Resize
      height: 320
      width: 320
      always_apply: true
    - _target_: albumentations.RandomBrightnessContrast
      p: 0.5
    - _target_: albumentations.RandomRotate90
      p: 0.25
    - _target_: albumentations.ShiftScaleRotate
      scale_limit: 0.05
      rotate_limit: 10
      border_mode: 0
      p: 1.0
    - _target_: src.data.transforms.GrayToRGB
    - _target_: albumentations.Normalize
      max_pixel_value: 1.0
      always_apply: true
    - _target_: albumentations.pytorch.ToTensorV2
    anchor_transforms:
    - _target_: albumentations.Resize
      height: 320
      width: 256
      always_apply: true
    - _target_: src.data.transforms.GrayToRGB
    - _target_: albumentations.Normalize
      max_pixel_value: 1.0
      always_apply: true
    - _target_: albumentations.pytorch.ToTensorV2
    anchor: canonical_chest
  loader:
    batch_size: 8
    num_workers: 8
model:
  module:
    _target_: src.modules.AlignmentModel
    backbone: resnet18
    loss_network: densenet121
    grid_shape:
    - 3
    - 320
    - 256
    backbone_kwargs:
      pretrained: true
      global_pool: avg
      num_classes: 0
      drop_rate: 0.1
    loss_network_kwargs:
      checkpoint_path: /home/orogov/smbmount/a_galichin/experiments/xvision/checkpoints/V1_nih14_densenet121_320x320_mbce_bs64/epoch=5-val_loss=0.205.ckpt
loss:
  _target_: src.losses.PerceptualLoss
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0001
  weight_decay: 0.0001
lr_scheduler:
  _target_: torch.optim.lr_scheduler.LinearLR
  start_factor: 1.0
  end_factor: 0.01
  total_iters: 20

[2023-03-10 18:05:04,722][src.train][INFO] - Number of devices = 2 > 1. Setting n_threads = 0 and disabling opencl to prevent deadlocks.
[2023-03-10 18:05:05,376][src.data.dataset][INFO] - Dataset size: 112120
[2023-03-10 18:05:05,400][src.data.dataset][INFO] - Prepared canonical chest from : /home/orogov/smbmount/from_DGX/cxr14-2/canonical_chest.png
[2023-03-10 18:05:05,607][timm.models.helpers][INFO] - Loading pretrained weights from url (https://download.pytorch.org/models/resnet18-5c106cde.pth)
[2023-03-10 18:05:05,785][src.modules][INFO] - Loading checkpoint from: /home/orogov/smbmount/a_galichin/experiments/xvision/checkpoints/V1_nih14_densenet121_320x320_mbce_bs64/epoch=5-val_loss=0.205.ckpt
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
experiment_path: /home/orogov/smbmount/a_galichin/experiments/alignment
experiment_name: nih_resnet18_densenet121_320x320_perceptual_bs16
trainer:
  devices:
  - 0
  - 1
  strategy: ddp
  precision: 32
  max_epochs: 3
data:
  dataset:
    _target_: src.data.dataset.ChestXRayAlignmentDataset
    main_dir: /home/orogov/smbmount/from_DGX/cxr14-2
    transforms:
    - _target_: albumentations.Resize
      height: 320
      width: 320
      always_apply: true
    - _target_: albumentations.RandomBrightnessContrast
      p: 0.5
    - _target_: albumentations.RandomRotate90
      p: 0.25
    - _target_: albumentations.ShiftScaleRotate
      scale_limit: 0.05
      rotate_limit: 10
      border_mode: 0
      p: 1.0
    - _target_: src.data.transforms.GrayToRGB
    - _target_: albumentations.Normalize
      max_pixel_value: 1.0
      always_apply: true
    - _target_: albumentations.pytorch.ToTensorV2
    anchor_transforms:
    - _target_: albumentations.Resize
      height: 320
      width: 256
      always_apply: true
    - _target_: src.data.transforms.GrayToRGB
    - _target_: albumentations.Normalize
      max_pixel_value: 1.0
      always_apply: true
    - _target_: albumentations.pytorch.ToTensorV2
    anchor: canonical_chest
  loader:
    batch_size: 8
    num_workers: 8
model:
  module:
    _target_: src.modules.AlignmentModel
    backbone: resnet18
    loss_network: densenet121
    grid_shape:
    - 3
    - 320
    - 256
    backbone_kwargs:
      pretrained: true
      global_pool: avg
      num_classes: 0
      drop_rate: 0.1
    loss_network_kwargs:
      checkpoint_path: /home/orogov/smbmount/a_galichin/experiments/xvision/checkpoints/V1_nih14_densenet121_320x320_mbce_bs64/epoch=5-val_loss=0.205.ckpt
loss:
  _target_: src.losses.PerceptualLoss
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0001
  weight_decay: 0.0001
lr_scheduler:
  _target_: torch.optim.lr_scheduler.LinearLR
  start_factor: 1.0
  end_factor: 0.01
  total_iters: 20

[2023-03-10 18:05:11,104][src.train][INFO] - Number of devices = 2 > 1. Setting n_threads = 0 and disabling opencl to prevent deadlocks.
[2023-03-10 18:05:11,724][src.data.dataset][INFO] - Dataset size: 112120
[2023-03-10 18:05:11,747][src.data.dataset][INFO] - Prepared canonical chest from : /home/orogov/smbmount/from_DGX/cxr14-2/canonical_chest.png
[2023-03-10 18:05:11,950][timm.models.helpers][INFO] - Loading pretrained weights from url (https://download.pytorch.org/models/resnet18-5c106cde.pth)
[2023-03-10 18:05:12,112][src.modules][INFO] - Loading checkpoint from: /home/orogov/smbmount/a_galichin/experiments/xvision/checkpoints/V1_nih14_densenet121_320x320_mbce_bs64/epoch=5-val_loss=0.205.ckpt
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
[2023-03-10 18:05:15,074][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2023-03-10 18:05:15,083][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2023-03-10 18:05:15,083][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

[2023-03-10 18:05:15,085][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
Missing logger folder: /home/orogov/smbmount/a_galichin/experiments/alignment/logs/nih_resnet18_densenet121_320x320_perceptual_bs16/lightning_logs
Missing logger folder: /home/orogov/smbmount/a_galichin/experiments/alignment/logs/nih_resnet18_densenet121_320x320_perceptual_bs16/lightning_logs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name   ┃ Type           ┃ Params ┃
┡━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ _model │ AlignmentModel │ 18.3 M │
│ 1 │ _loss  │ PerceptualLoss │      0 │
└───┴────────┴────────────────┴────────┘
Trainable params: 11.3 M                                                        
Non-trainable params: 7.0 M                                                     
Total params: 18.3 M                                                            
Total estimated model params size (MB): 73                                      
[2023-03-10 18:05:17,732][src.models][INFO] - Calculating anchor features: Start
[2023-03-10 18:05:17,736][src.models][INFO] - Calculating anchor features: Start
[2023-03-10 18:05:18,245][src.models][INFO] - Calculating anchor features: Done
[2023-03-10 18:05:18,724][src.models][INFO] - Calculating anchor features: Done
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:4277: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:4215: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:4277: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:4215: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
Error executing job with overrides: []
Traceback (most recent call last):
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 36, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 90, in launch
    return function(*args, **kwargs)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 624, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1061, in _run
    results = self._run_stage()
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1140, in _run_stage
    self._run_train()
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1163, in _run_train
    self.fit_loop.run()
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.on_advance_end()
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 295, in on_advance_end
    self.trainer._call_callback_hooks("on_train_epoch_end")
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1343, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 312, in on_train_epoch_end
    self._save_topk_checkpoint(trainer, monitor_candidates)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 371, in _save_topk_checkpoint
    self._save_none_monitor_checkpoint(trainer, monitor_candidates)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 657, in _save_none_monitor_checkpoint
    self._save_checkpoint(trainer, filepath)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 374, in _save_checkpoint
    trainer.save_checkpoint(filepath, self.save_weights_only)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1904, in save_checkpoint
    self._checkpoint_connector.save_checkpoint(filepath, weights_only=weights_only, storage_options=storage_options)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 540, in save_checkpoint
    self.trainer.strategy.save_checkpoint(_checkpoint, filepath, storage_options=storage_options)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 466, in save_checkpoint
    self.checkpoint_io.save_checkpoint(checkpoint, filepath, storage_options=storage_options)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/lightning_lite/plugins/io/torch_io.py", line 54, in save_checkpoint
    _atomic_save(checkpoint, path)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/lightning_lite/utilities/cloud_io.py", line 69, in _atomic_save
    f.write(bytesbuffer.getvalue())
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/fsspec/core.py", line 122, in __exit__
    self.close()
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/fsspec/core.py", line 142, in close
    f.close()
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/fsspec/implementations/local.py", line 358, in close
    return self.f.close()
InterruptedError: [Errno 4] Interrupted system call

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run_training.py", line 10, in _main
    train(cfg)
  File "/home/orogov/smbmount/a_galichin/alignment_module/src/train.py", line 89, in train
    trainer.fit(model)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 582, in fit
    call._call_and_handle_interrupt(
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 59, in _call_and_handle_interrupt
    trainer.strategy.reconciliate_processes(traceback.format_exc())
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 461, in reconciliate_processes
    raise DeadlockDetectedException(f"DeadLock detected from rank: {self.global_rank} \n {trace}")
pytorch_lightning.utilities.exceptions.DeadlockDetectedException: DeadLock detected from rank: 0 
 Traceback (most recent call last):
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 36, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 90, in launch
    return function(*args, **kwargs)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 624, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1061, in _run
    results = self._run_stage()
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1140, in _run_stage
    self._run_train()
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1163, in _run_train
    self.fit_loop.run()
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.on_advance_end()
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 295, in on_advance_end
    self.trainer._call_callback_hooks("on_train_epoch_end")
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1343, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 312, in on_train_epoch_end
    self._save_topk_checkpoint(trainer, monitor_candidates)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 371, in _save_topk_checkpoint
    self._save_none_monitor_checkpoint(trainer, monitor_candidates)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 657, in _save_none_monitor_checkpoint
    self._save_checkpoint(trainer, filepath)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 374, in _save_checkpoint
    trainer.save_checkpoint(filepath, self.save_weights_only)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1904, in save_checkpoint
    self._checkpoint_connector.save_checkpoint(filepath, weights_only=weights_only, storage_options=storage_options)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 540, in save_checkpoint
    self.trainer.strategy.save_checkpoint(_checkpoint, filepath, storage_options=storage_options)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 466, in save_checkpoint
    self.checkpoint_io.save_checkpoint(checkpoint, filepath, storage_options=storage_options)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/lightning_lite/plugins/io/torch_io.py", line 54, in save_checkpoint
    _atomic_save(checkpoint, path)
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/lightning_lite/utilities/cloud_io.py", line 69, in _atomic_save
    f.write(bytesbuffer.getvalue())
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/fsspec/core.py", line 122, in __exit__
    self.close()
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/fsspec/core.py", line 142, in close
    f.close()
  File "/home/a_galichin/anaconda3/envs/pytorch/lib/python3.8/site-packages/fsspec/implementations/local.py", line 358, in close
    return self.f.close()
InterruptedError: [Errno 4] Interrupted system call


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
